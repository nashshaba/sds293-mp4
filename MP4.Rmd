---
title: "SDS/CSC 293 Mini-Project 4: CART"
author: "Group 18: Nashshaba Nawaz & Irene Ryan"
date: "Wednesday, April 17^th^, 2019"
output:
  html_document:
    highlight: tango
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: true
    df_print: kable
---

```{r setup, include=FALSE}
# Load all your packages here:
library(tidyverse)
library(GGally)
library(rpart)
library(yardstick)

# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 16/2, fig.height = 9/2
)

# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

You will be fitting CART models to the data from the [Ghouls, Goblins, and Ghosts... Boo!](https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo/){target="_blank"} Kaggle competition. The competition's score for leaderboard purposes is the "Categorization Accuracy". However you will **NOT** be making any submissions to Kaggle.



***



# Load data

Read in data provided by Kaggle for this competition. They are organized in the `data/` folder of this RStudio project:

```{r}
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
```


## Look at your data!

Always, ALWAYS, **ALWAYS** start by looking at your raw data. This gives you visual sense of what information you have to help build your predictive models. To get a full description of each variable, read the data dictionary in the `data_description.txt` file in the `data/` folder.

Note that the following code chunk has `eval = FALSE` meaning "don't evaluate this chunk with knitting" because `.Rmd` files won't knit if they include a `View()`:

```{r, eval = FALSE}
View(training)
glimpse(training)

View(test)
glimpse(test)
```

In particular, pay close attention to the variables and variable types in the
`sample_submission.csv`. Your submission must match this exactly.

```{r}
glimpse(sample_submission)
```



***



# Minimally viable product

Perform the following exploratory data analyses:

## Univariate explorations

**Categorical predictor**: Create a visualization of the categorical predictor variable `color`.

```{r}
ggplot(data=training,aes(x=color, fill=color))+
  geom_bar(color="grey1",show.legend = FALSE)+
  labs(title="Distributuion of the categorical predictor variable", x="Dominant Color of the Creature") +
  scale_fill_manual(values=c("grey1", "darkred", "blue","snow2","lawngreen","ghostwhite"))
```


**Outcome variable**: Create a visualization of the categorical outcome variable `type`.

```{r}
ggplot(data=training,aes(x=type))+
  geom_bar()+
  labs(title="Distributuion of the categorical outcome variable", x="Creature Type")
```


## Mutlivariate explorations

**Numerical predictors**: Create a visualization of the relationship of all four numerical predictor variables at once (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`) using the `ggpairs()` function from the `GGally` [package](http://ggobi.github.io/ggally/#ggallyggpairs). 

```{r}
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length","has_soul"), columnLabels = c("Avg bone length", "% of rotting flesh", "Avg hair length","% of soul"))+
  labs(title = "Relationship between all four numerical predictor variables")


```

**Relationship of categorical predictor and outcome variable**: Create a visualization of the relationship between the categorical outcome variable `type` and any predictor varible of your choosing.

```{r}
ggplot(training, aes(x = color, fill = type)) +
  geom_bar(position = "fill") +
  labs(fill = "Types of creature", title = " Figure 1: Distribution of color of creature split by creature type", y="Proportion") 
```

```{r}
ggplot(data=training,aes(x=type, y=rotting_flesh))+
  geom_boxplot()+
  labs(x="Creature Type", y="% of rotting flesh in the creature",title = "Figure 2: Distribution of percentage of rotting flesh split by type of creature") 
```



***



# Due diligence

1. Fit a CART where:
    * You use only the numerical predictors.
    * The maximum depth of the tree is 5.
    * You use the default "complexity parameter" 
1. Plot the tree.
1. Make predictions `type_hat` on the `training` data. Hint compare the output of `predict(model_CART, type = "prob")` and `predict(model_CART, type = "class")`.
1. Compute the "classification accuracy".

```{r, fig.height = 9, fig.width=16}

# Fit CART model using only numerical predictors, in this case for classification
model_formula_1 <- as.formula(type ~ bone_length + rotting_flesh + hair_length + has_soul)
tree_parameters_1 <- rpart.control(maxdepth = 5)
model_CART_1 <- rpart(model_formula_1, data = training, control = tree_parameters_1)

# Plot CART model
{plot(model_CART_1, margin=0.25)
text(model_CART_1, use.n = TRUE)

title("Predicting type of creature using bone length,% of rotting flesh, hair length, & % of soul")
box()}

# Get fitted probabilities for each class on train
type_hat_prob <- model_CART_1 %>%
  predict(type = "prob", newdata = training)%>%
  # Convert matrix object to data frame:
  as_tibble()

#Get fitted ys, where highest probability wins and ties are broken at random
type_hat_class <- model_CART_1 %>%
  predict(type = "class", newdata = training) ##%>%
  # Convert matrix object to data frame:
  ##enframe()
type_hat_class_temp <- model_CART_1 %>%
  predict(type = "class", newdata = training) %>%
  as.tibble()
#Rename value to type_hat
colnames(type_hat_class_temp)[1]<-"type_hat"
#Convert id colum to be numeric
#type_hat_class$id <- as.numeric(as.character(type_hat_class$id))


```

```{r}
#If we look at the output of type_hat_prob and type_hat_class then we can see that type_hat_prob returns fitted probabilities for each class whereas type_hat_class returns fitted ys by picking a winner based on the maximum probability. In other words, the highest probability wins.

head(type_hat_prob, n=2)
head(type_hat_class_temp, n=2)

```



```{r}
# Save predictions in training data frame
training <- training %>%
      mutate(type_hat = type_hat_class)  

#First way to compute CA
#CA1 <- sum(training$type == training$type_hat)/nrow(training)
#CA1

#Second way to compute CA
CA<-training %>%
  mutate(type_hat = training$type_hat,
         true = ifelse(type_hat==type, 1, 0))%>%
  summarise(CA=mean(true==1))

CA
```






***


# Reaching for the stars

Note that the $\alpha$ complexity parameter is the `cp` argument to `rpart.control()`.

1. Reusing the MP1 solutions code, for the range of `alpha` complexity parameters in the `alpha_df` data frame, return an estimate of the accuracy/error that Kaggle would return.
1. Plot the relationship between the alpha complexity parameter and accuracy.
1. Using the optimal $\alpha^*$ complexity parameter, write a `submission.csv` suitable for submission to Kaggle.


```{r}
set.seed(77)

training <- training %>% 
  sample_frac(1) %>% 
  mutate(fold = rep(1:5, length = n())) %>% 
  arrange(fold)

alpha_df <- tibble(
  alpha = seq(from = 0, to = 0.05, length = 100),
  accuracy = 0
)

for(i in 1:nrow(alpha_df)){
  alpha <- alpha_df$alpha[i]
  
  CA <- rep(0, 5)
  for(j in 1:5){
    pretend_training <- training %>% 
      filter(fold != j)
    pretend_test <- training %>% 
      filter(fold == j)
    
    # Fit model on pretend training
    model_1 <- as.formula(type ~ bone_length + rotting_flesh + hair_length + has_soul)
    tree_parameters <- rpart.control(maxdepth = 5, cp = alpha)
    model_CART <- rpart(model_1, data = pretend_training, control = tree_parameters)
    
    # Make predictions
    predicted_pretend_test <- model_CART %>%
      predict(type = "class", newdata = pretend_test)##%>%
      ##enframe()
    
     # Save predictions in pretend_test data frame
    pretend_test<-pretend_test%>%
      mutate(y_hat = predicted_pretend_test)

    CA[j] <- pretend_test %>%
      mutate(truth=ifelse(type==y_hat, 1, 0))%>%
      summarise(CA = mean(truth==1))%>%
      pull(CA)

    
    #CA[j] <- sum(pretend_test$type == pretend_test$y_hat)/nrow(pretend_test) 
  }
  
  alpha_df$accuracy[i] <- mean(CA)
}
```


#Visualization
```{r}
ggplot(alpha_df, aes(x = alpha, y = accuracy)) +
  geom_line()
```


##Get optimal alpha
```{r}
rearr_alpha_df <- alpha_df%>%
  arrange(desc(accuracy))

optimal_alpha <- rearr_alpha_df$alpha[[1]]

optimal_alpha
```
Optimal alpha level = 0.0055555556

```{r}
optimal_model <- as.formula(type ~ bone_length + rotting_flesh + hair_length + has_soul)
tree_parameters <- rpart.control(maxdepth = 5, cp = .0056)
model_CART_optimal <- rpart(optimal_model, data = training, control = tree_parameters)
    
predicted_points_optimal <- model_CART_optimal %>%
    predict(type = "class", newdata = test)

#Save predictions in test data frame
    test<-test%>%
      mutate(type_hat = predicted_points_optimal)
    
#Submission data frame
    submission <- test %>%
      select(id,type_hat)

write_csv(submission, path = "data/submission.csv")
```

***



# Point of diminishing returns

* Use one-hot-enconding to fit a predictive CART model using the categorical variable `color` and plot the tree. Set the maximum depth of the tree to 5 and use the default "complexity parameter".
* No need to generate an estimate of the accuracy that Kaggle would return. 

![](one_hot_encoding.png)

```{r}
training<-training%>%
  mutate(black = ifelse(color == "black", 1, 0),
         blood = ifelse(color == "blood", 1, 0),
         blue = ifelse(color == "blue", 1, 0),
         clear = ifelse(color == "clear", 1, 0),
         white = ifelse(color == "white", 1, 0))

# Fit CART model, in this case for classification
model_formula_2 <- as.formula(type ~ black+blue+blood+clear+white)
model_CART_2 <- rpart(model_formula_2, data = training, control = tree_parameters_1)

# Plot CART model
{plot(model_CART_2, margin=0.25)
text(model_CART_2, use.n = TRUE)

title("Predicting type of creature using the categorical variable, color")
box()}
```



